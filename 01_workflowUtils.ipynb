{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.metrics import mean_poisson_deviance\n",
        "import numpy as np\n"
      ],
      "metadata": {
        "id": "SiwBpr2ff35m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "db7mjK3aZyVl"
      },
      "outputs": [],
      "source": [
        "\n",
        "np.seterr(divide = 'ignore') \n",
        "\n",
        "def ImportData(FreqFile, SevFile):\n",
        "  dfFreq=pd.read_csv(FreqFile, header='infer')\n",
        "  dfSev=pd.read_csv(SevFile, header='infer')\n",
        "  dfFreq[\"IDpol\"] = dfFreq[\"IDpol\"].astype(int)\n",
        "  dfFreq.set_index(\"IDpol\", inplace=True)\n",
        "  dfSev = dfSev.groupby(\"IDpol\").sum()\n",
        "  df = dfFreq.join(dfSev, how=\"left\")\n",
        "  df[\"ClaimAmount\"].fillna(0, inplace=True)\n",
        "  # unquote string fields\n",
        "  for column_name in df.columns[df.dtypes.values == object]:\n",
        "        df[column_name] = df[column_name].str.strip(\"'\")\n",
        "  df[\"PurePremium\"] = df[\"ClaimAmount\"] / df[\"Exposure\"]\n",
        "  df[\"Frequency\"] = df[\"ClaimNb\"] / df[\"Exposure\"]\n",
        "  df[\"Severity\"] = df[\"ClaimAmount\"] / np.fmax(df[\"ClaimNb\"], 1)\n",
        "  df[\"Log_PurePremium\"]=np.log(df['PurePremium'])\n",
        "  df[\"Log_Frequency\"]=np.log(df['Frequency'])\n",
        "  df[\"Log_Severity\"]=np.log(df['Severity'])\n",
        "  df[\"Log_ClaimNb\"]=np.log(df['ClaimNb'])\n",
        "  df[\"Log_ClaimAmount\"]=np.log(df['ClaimAmount'])\n",
        "  df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "  return df\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def ExploratoryAnalysis(df, mlflow, mountpoint, targetColumns, title):\n",
        "  for targetColumn in targetColumns:\n",
        "    print(f\"Profiling dataset for {targetColumn}...\")\n",
        "    my_report  = sweetviz.analyze([df,'Train'], target_feat=targetColumn)\n",
        "    ReportPath=f'{mountPoint}/MyDrive/ColabNotebooks/InsuranceClaims/Reports/{targetColumn}_profile_Report.html'\n",
        "    my_report.show_html(ReportPath)\n",
        "    mlflow.log_artifact(ReportPath, title)"
      ],
      "metadata": {
        "id": "LgpI8gVStY0b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def FeatureEngineering_Pre(df):\n",
        "  df2=df.copy()\n",
        "  df2.loc[ (df2[\"Region\"]==\"Auvergne\") | \\\n",
        "          (df2[\"Region\"]==\"Limousin\") | \\\n",
        "          (df2[\"Region\"]==\"Corse\") |\n",
        "          (df2[\"Region\"]==\"Champagne-Ardenne\") |\n",
        "          (df2[\"Region\"]==\"Alsace\") |\n",
        "          (df2[\"Region\"]==\"Franche-Comte\") \n",
        "          ,\"Region\"]=\"Other\"\n",
        "\n",
        "  df2.loc[ (df2[\"VehBrand\"]==\"B13\") | \\\n",
        "          (df2[\"VehBrand\"]==\"B14\") \n",
        "          ,\"VehBrand\"]=\"B13+B14\"\n",
        "\n",
        "\n",
        "  df2.loc[ (df2[\"DrivAge\"]>=83) ,\"DrivAge\"]=83\n",
        "\n",
        "  df2.loc[ (df2[\"VehAge\"]>=50) ,\"VehAge\"]=50\n",
        "\n",
        "  df2.loc[(df2[\"ClaimAmount\"] == 0) & (df2[\"ClaimNb\"] >= 1), \"ClaimNb\"] = 0\n",
        "\n",
        "\n",
        "  df2[\"ClaimNb\"] = df2[\"ClaimNb\"].clip(upper=4)\n",
        "  df2[\"Exposure\"] = df2[\"Exposure\"].clip(upper=1)\n",
        "  df2[\"ClaimAmount\"] = df2[\"ClaimAmount\"].clip(upper=200000)\n",
        "\n",
        "  return df2"
      ],
      "metadata": {
        "id": "GBxM3pxWxrsD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import FunctionTransformer, OneHotEncoder\n",
        "from sklearn.preprocessing import StandardScaler, KBinsDiscretizer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "\n",
        "def FeatureEngineering(df):\n",
        "  df2=FeatureEngineering_Pre(df)\n",
        "\n",
        "  log_scale_transformer = make_pipeline(\n",
        "    FunctionTransformer(func=np.log), StandardScaler()\n",
        "  )\n",
        "\n",
        "  preprocessor = ColumnTransformer(\n",
        "      transformers=[\n",
        "          (\"binned_numeric\", KBinsDiscretizer(n_bins=10), [\"VehAge\", \"DrivAge\"]),\n",
        "          (\n",
        "              \"onehot_categorical\",\n",
        "              OneHotEncoder(),\n",
        "              [\"VehBrand\", \"VehPower\", \"VehGas\", \"Region\", \"Area\"],\n",
        "          ),\n",
        "          (\"passthrough_numeric\", \"passthrough\", [\"BonusMalus\"]),\n",
        "          (\"log_scaled_numeric\", log_scale_transformer, [\"Density\"])\n",
        "\n",
        "      ],\n",
        "      remainder=\"drop\",\n",
        "  )\n",
        "\n",
        "  clf = Pipeline(steps=[('preprocessor', preprocessor)])\n",
        "\n",
        "  X = clf.fit_transform(df)\n",
        "\n",
        "  x1=clf.named_steps['preprocessor'].transformers_[0][1].get_feature_names_out()\n",
        "  x2=clf.named_steps['preprocessor'].transformers_[1][1].get_feature_names_out()\n",
        "  columnNames=np.concatenate((x1, x2, np.array([\"BonusMalus\", \"Density\"])), axis=0)\n",
        "\n",
        "  return X, df2, columnNames\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "XoLtPdJw2fk3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def score_model_metrics(df_test, X_test, VariableWeight, VarTarget, estimator):\n",
        "  \n",
        "  if len(estimator) == 1:\n",
        "    y_pred = estimator[0].predict(X_test)\n",
        "  else:\n",
        "    y_pred=estimator[0].predict(X_test) * estimator[1].predict(X_test)\n",
        "  return (\n",
        "            round(\n",
        "              mean_absolute_error(\n",
        "              df_test[VarTarget], y_pred, sample_weight=df_test[VariableWeight]\n",
        "              ),3\n",
        "            ),\n",
        "            round(\n",
        "            mean_squared_error(\n",
        "                df_test[VarTarget], y_pred, sample_weight=df_test[VariableWeight]\n",
        "            ),3)\n",
        "          )"
      ],
      "metadata": {
        "id": "NcO3OkqDvQEe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import auc\n",
        "\n",
        "def lorenz_curve(y_true, y_pred, exposure):\n",
        "    y_true, y_pred = np.asarray(y_true), np.asarray(y_pred)\n",
        "    exposure = np.asarray(exposure)\n",
        "    y_true=y_true.astype(float)\n",
        "    # order samples by increasing predicted risk:\n",
        "    ranking = np.argsort(y_pred)\n",
        "    ranked_exposure = exposure[ranking]\n",
        "    ranked_pure_premium = y_true[ranking]\n",
        "    cumulated_claim_amount = np.cumsum(ranked_pure_premium * ranked_exposure)\n",
        "    cumulated_claim_amount /= cumulated_claim_amount[-1]\n",
        "    cumulated_samples = np.linspace(0, 1, len(cumulated_claim_amount))\n",
        "    return cumulated_samples, \\\n",
        "           cumulated_claim_amount,\\\n",
        "           1 - 2 * auc(cumulated_samples, cumulated_claim_amount)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yDz7BAQ-gwiR",
        "outputId": "9d9e139c-bd7a-49b3-b06f-874a89674d19"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "def algorithm_pipeline(X_train_data, X_test_data, y_train_data, y_test_data, \n",
        "                       model, param_grid, cv=10, scoring_fit='neg_mean_squared_error',\n",
        "                       do_probabilities = False):\n",
        "    gs = GridSearchCV(\n",
        "        estimator=model,\n",
        "        param_grid=param_grid, \n",
        "        cv=cv, \n",
        "        n_jobs=-1, \n",
        "        scoring=scoring_fit,\n",
        "        verbose=2\n",
        "    )\n",
        "    fitted_model = gs.fit(X_train_data, y_train_data)\n",
        "    \n",
        "    if do_probabilities:\n",
        "      pred = fitted_model.predict_proba(X_test_data)\n",
        "    else:\n",
        "      pred = fitted_model.predict(X_test_data)\n",
        "    \n",
        "    return fitted_model, pred"
      ],
      "metadata": {
        "id": "dWLzebxb-QTF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "def algorithm_pipeline_random(X_train_data, X_test_data, y_train_data, y_test_data, \n",
        "                       model, param_grid, cv=10, scoring_fit='neg_mean_squared_error',\n",
        "                       do_probabilities = False, iterations=10000):\n",
        "    gs = RandomizedSearchCV(\n",
        "        estimator=model,\n",
        "        param_distributions=param_grid, \n",
        "        cv=cv, \n",
        "        n_jobs=-1, \n",
        "        scoring=scoring_fit,\n",
        "        verbose=2,\n",
        "        n_iter=iterations\n",
        "    )\n",
        "    #gs.set_params(param_grid)\n",
        "    fitted_model = gs.fit(X_train_data, y_train_data)\n",
        "    \n",
        "    if do_probabilities:\n",
        "      pred = fitted_model.predict_proba(X_test_data)\n",
        "    else:\n",
        "      pred = fitted_model.predict(X_test_data)\n",
        "    \n",
        "    return fitted_model, pred"
      ],
      "metadata": {
        "id": "SkFm_85BP9I8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}